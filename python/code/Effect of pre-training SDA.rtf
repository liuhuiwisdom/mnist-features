{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl320

\f0\fs24 \cf0 \expnd0\expndtw0\kerning0
Effect of pre-training\
\
Without pre-training, fine-tuning only (minibatch size=100):\
Best model\'92s test error = 2.08% (pretty good even without pre-training!)\
\
With pre-training over 5 epochs, plus fine-tuning (minibatch size=100):\
Best model\'92s test error = 1.85% (slightly better, but not spectacularly)\
\
With pre-training over 15 epochs, plus fine-tuning (minibatch size=100):\
Best model\'92s test error = 1.8%\'a0\
\
With pre-training over 30 epochs, plus fine-tuning (minibatch size=100):\
Best model\'92s test error = 1.69%\
\
With pre-training over 100 epochs, plus fine-tuning (minibatch size=100):\
Best model\'92s test error = 1.7%\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\expnd0\expndtw0\kerning0
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\expnd0\expndtw0\kerning0
\
\pard\pardeftab720\sl320
\cf0 \expnd0\expndtw0\kerning0
Without pre-training, only fine-tuning (minibatch size=1):\
Best model\'92s test error = 1.5%\expnd0\expndtw0\kerning0
\
\
With pre-training over 15 epochs, plus fine-tuning (minibatch size=1):\
Best model\'92s test error = 1.46%\
}